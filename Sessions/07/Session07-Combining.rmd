---
title: "SCRGOT 2023 Coder Upgrade Session 07 - Combining single cell datasets"
author: "Ryan Roberts"
date: "4/16/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(rrrSingleCellUtils)
library(Seurat)
library(parallel)

theme_set(theme_classic())
```

## Introduction to the datasets

Within the accompanying "Datasets" folder, you will find two folders, each containing a collection of single cell datasets. For the sake of simplicity, all of these datasets are count matrices generated by 10x's cellranger pipeline (like the data you worked with yesterday in the "Intro to Seurat" session). 

The "PatientTumors" folder contains data from several primary bone tumors, processed to produce single cell libraries directly from patients. These data were downloaded from the GEO database (GSE162454). Each of these samples was collected from a different pateint and processed on a different day, sometimes in a different lab, from all of the others. All of the cells in every sample are human.

The "XenoTumors" folder contains data from a mouse experiment, where immunodeficient mice were injected by tail vein with human bone tumor cells to produce metastatic lung tumors. Lungs were then harvested at several timepoints and processed on the day of harvest to produce single cell libraries that were sequenced. These samples each contain the same tumor cells, harvested at different stages along the path toward metastatic colonization, but also contain the surrounding lung cells (isolated using a niche labeling technique). Importantly, it contains both mouse and human cells--the sequenced libraries were aligned to a mixed human/mouse genome.

Start by practicing your skills learned yesterday in the Intro session to take one of the datasets (one sample) from the count tables provided in the PatientTumors folder to a UMAP plot. HINT: Read10x, CreateSeuratObject, PercentageFeatureSet, VlnPlot, subset, NormalizeData, FindVariableFeatures, ScaleData, RunPCA, FindNeighbors, FindClusters, RunUMAP, DimPlot.

```{r}
t1 <- Read10X("Datasets/PatientTumors/OS_5") %>%
    CreateSeuratObject() %>%
    PercentageFeatureSet(pattern = "^MT-", col.name = "percent.mt")

VlnPlot(t1, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"))

t1 <- subset(t1, subset = nCount_RNA < 50000 & percent.mt < 20) %>%
    NormalizeData() %>%
    FindVariableFeatures() %>%
    ScaleData() %>%
    RunPCA() %>%
    FindNeighbors() %>%
    FindClusters() %>%
    RunUMAP(dims = 1:30)

DimPlot(t1, pt.size = 1, label = TRUE) +
    ggtitle("Sample OS_5 - initial clustering") +
    coord_fixed()
```

# Merging datasets

The first step toward combining multiple datasets is to move all of the normalized data (from all of the objects that you want to combine) into a single Seurat object. This is best accomplished by creating a list of individual Seurat objects, then using the merge command to combine them into one. 

You need to have a little forethought in the way that you do this and create metadata variables that will track the origins of each cell. If you don't do this, when the cells are all combined together, you won't be able to tell which cells are which.

Step-by-step instructions:
1. Create a list containing individual Seurat objects representing each of the datasets in the "PatientTumors" folder. You have three options for how you might approach this:
    - Acceptable: create each of the Seurat objects separately, then combine them into a list using the list() command.
    - Better: create a for() loop that loops through code to create the Seurat objects, adding them to the list object without an intermediate (the individual Seurat object).
    - Best: create a dataframe, tibble, or list of vectors containing all of the variables that you will need to create the list of Seurat objects and a function to create a normalized Seurat object and add metadata, then use lapply to map the dataframe to the function, building the list of Seurat objects in a single step. (This is the most computationally efficient and flexible way to do it, and you can reuse the function you build over and over).
2. Merge these into a single Seurat object using the merge() command. Candidly, this is a super awkward function, so I'll lay it out for you the way it will look:
    
    combined_object <- merge(seurat_list[[1]], seurat_list[2:length(seurat_list)], add.cell.ids = sample_list)

The first element of the merge() is the first object that you want to combine, The second is "the rest" of the elements that you want to combine, and the add.cell.ids is a prefix added to the cell barcodes, in case some of them are the same from one sample to another (they won't get mistakenly put together or crash the operation because they're not unique if you use this option).
3. Perform all the usual operations to process your dataset, just like you did for the individual Seurat objects--FindVariableFeatures, ScaleData, yada yada yada through RunUMAP.
4. Now plot the data to see what you have. This is an important decision point. Based on the nature of the samples/dataset, the details surrounding how they were created, and the scientific question you intend to ask, you need to decide whether or not this will work without alignment, or whether it needs to be aligned (and what you'll use to attempt alignment/batch correction, if you do choose to use it). To check out how well aligned (or not) your data is, use the group.by and split.by options of DimPlot to use that metadata element you added earlier to either color by source or panel by source, respectively.
5. Are your samples clustering by sample or by cell type or a little of both?
6. In this example, we have tumor cells that seem very different one from the next, but stromal cells that might be similar. Let's separate out the tumor cells from the stromal cells. I've done the work for you already by identifying the tumor cells, which you can isolate this way:

    tumor_cells <- readRDS("Datasets/PatientTumors/tumor_cells.rds")
    Idents(t1, cells = tumor_cells) <- Tumor
    tumor <- subset(t1, cells = tumor_cells)

```{r}
# Build a list of samples to import
sample_list <- c("OS_1", "OS_2", "OS_3", "OS_4", "OS_5", "OS_6")

# Write a function to import a sample (be sure to label where it came from)
create_seurat <- function(x) {
    message(paste("Loading", x, "..."))
    s <- Read10X(paste0("Datasets/PatientTumors/", x)) %>%
    CreateSeuratObject(min_cells = 5, min_features = 800) %>%
    PercentageFeatureSet(pattern = "^MT-", col.name = "percent.mt")
    s$src <- x
    message(paste("Sample", x, "is loaded."))
    return(s)
}

# Map the sample_list to the create_seurat function with lapply to generate
# a list of Seurat objects
tumors <- mclapply(sample_list, create_seurat, mc.cores = 4L)
names(tumors) <- sample_list

violins <- list()
for(t in names(tumors)) {
    violins[[t]] <- VlnPlot(tumors[[t]],
        features = c("nFeature_RNA", "nCount_RNA", "percent.mt")) +
        plot_annotation(title = t,
            theme = theme(plot.title = element_text(size = 16)))
}

wrap_plots(violins, ncol = 3)

qc <- data.frame(
    sample = sample_list,
    ncount = c(70000, 60000, 30000, 70000, 80000, 60000),
    mtpercent = c(25, 25, 25, 25, 25, 25)
)

tum_cells <- readRDS("Datasets/PatientTumors/tumor_cells.rds") %>%
    stringr::str_extract("[ACTG]{16}-[:digit:]")

patient_tumor <- mclapply(seq_along(tumors), function(i) {
    x <- tumors[[i]] %>%
        subset(subset = nCount_RNA < qc$ncount[i] &
            percent.mt < qc$mtpercent[i]) %>%
        NormalizeData() %>%
        FindVariableFeatures() %>%
        ScaleData() %>%
        RunPCA() %>%
        FindNeighbors() %>%
        FindClusters() %>%
        RunUMAP(dims = 1:30)
    Idents(x) <- "Stroma"
    Idents(x, cells = tum_cells) <- "Tumor"
    return(x)
})
names(patient_tumor) <- names(tumors)

p <- mclapply(patient_tumor, function(x) {
    r_dim_plot(x)
}, mc.cores = 4L)

wrap_plots(p, ncol = 3)

p <- mclapply(patient_tumor, function(x) {
    r_feature_plot(x, features = "SATB2")
}, mc.cores = 4L)

wrap_plots(p, ncol = 3)

tum_cells <- c()
for(i in seq_along(tumors)) {
    tum_cells <- c(tum_cells, 
        CellSelector(p[[i]]))
}

dsample <- 500
pt_tumor <- lapply(patient_tumor, function(x) {
    s <- subset(x, cells = tum_cells)
    cell_list <- colnames(s)
    samp_number <- min(dsample, length(cell_list))
    cell_list <- sample(cell_list, samp_number)
    s <- subset(s, cells = cell_list)
    return(s)
})

pt_stroma <- lapply()

biggie <- merge(patient_tumor[[1]], patient_tumor[2:length(patient_tumor)],
    add.cell.ids = names(patient_tumor))

biggie_tumor <- subset(biggie, cells = tum_cells)

```

















































*NOTE: The procedures we will use in this exercise will only work if R is running in a Mac/UNIX/Linux environment. This is because the most efficient (and embarrasingly easy to implement) solutions for multi-core computing in R rely on a process called "forking", which Windows doesn't support. You're OK if you're working on a Windows machine to remote into the cluster, or if you're using a local windows machine to pass commands to a remote Linux machine (like in VSCode). If you need to run multi-core on a Windows machine, check out solutions like the doParallel package.

## Examine the data

The "Datasets" folder contains several single cell datasets downloaded from two different GEO entries. The names of the next level folders (GSEXXXXXX) are the GEO accession IDs. Within these are a series of folders named by the sample ID reported in the publication, each containing the three barcodes/features/matrix files produced by a cellranger alignment. The tibble created below consolidates key data elements associated with each sample.

The samples used to create these datasets were tumors taken from individuals diagnosed with a type of bone tumor (osteosarcoma). Osteosarcoma has several different histologic subtypes (the "path" element below). Primary tumors form in bones, while the lungs are the most common site of metastasis (the "type" argument). There was no additional processing/sorting of the tumor samples before sequencing, so the datasets contain both tumor cells and the surrounding stromal and immune components. The "nCount" column contains a simple pre-determined qc cutoff to reduce doublets.

Within these data are opportunites to address several different scientific questions. Think of some question that you might ask of such data, then select the datasets that you'd use to answer it (for this exercise, ideally 6-10 datasets). Delete or comment out the rows that you won't use, then run the code to create the tibble.

```{r define_data}
geo_data <- tribble(
    ~gse, ~id, ~nCount, ~path, ~type,
    "GSE152048", "BC5", 18000, "Conventional", "Primary",
    "GSE152048", "BC6", 25000, "Conventional", "Primary",
    # "GSE152048", "BC10", 25000, "Conventional", "Lung Met",
    "GSE152048", "BC11", 30000, "Conventional", "Primary",
    "GSE152048", "BC16", 70000, "Conventional", "Primary",
    # "GSE152048", "BC17", 40000, "Chondroblastic", "Lung Met",
    # "GSE152048", "BC20", 70000, "Chondroblastic", "Primary",
    # "GSE152048", "BC21", 50000, "Intraosseous", "Primary",
    # "GSE152048", "BC22", 50000, "Chondroblastic", "Primary",
    "GSE162454", "OS_1", 50000, "Conventional", "Primary",
    "GSE162454", "OS_2", 45000, "Conventional", "Primary",
    "GSE162454", "OS_3", 23000, "Conventional", "Primary",
    "GSE162454", "OS_4", 50000, "Conventional", "Primary",
    "GSE162454", "OS_5", 50000, "Conventional", "Primary",
    "GSE162454", "OS_6", 45000, "Conventional", "Primary"
)
```

## Practice 1: Simple parallelization with mclapply (from the parallel package)

We will create a list of Seurat objects containing your chosen datasets. We will start by loading and processing the objects 

1. Create a function that will utilize the variables from the tibble you created to convert the data matrices found in the "Datasets" folder into Seurat objects, subsetting for nCount_RNA < $path as a super basic QC. HINT: feel free to use the tenx_load_qc function from rrrSingleCellUtils to streamline the process.
2. Now map the data from the tibble into the function using lapply to create a list of Seurat objects.
3. Then wrap this code with a pair of system timestamps [Sys.time()] and calculate the difference to document the time it takes to perform the operation.
4. Once you have this working, copy that block of code and paste it at the end of the chunk. Then, simply change the "lapply" command to "mclapply" and add the mc.cores argument, with the number of cores set to those you requested in your session.
5. Determine how much time is saved by running the processes in parallel.
6. BONUS. Want to see how all of this is happening by monitoring the processes running on the cluster in real time? Open a separate shell running on the same node and run "top". Then, set the process in motion and observe how your code utilizes the different cores.
7. Now repeat this procedure to create a function that will process your Seurat objects (NormalizeData, FindVariableFeatures, ScaleData, RunPCA, FindNeighbors, FindClusters, RunUMAP), then plot the UMAPs. How much time do you save with this step by running it parallel?
8. BONUS. Restructure your code so that the data loading and processing operations occur in a single parallel computing operation, rather than two separate parallel computing operations. Embed a series of timestamps to benchmark the two approaches and compare the results. Does combining the two operations increase efficiency? Why or why not?
9. BONUS. Are the data objects created using the serial and the parallel processing approaches identical? Why or why not? (see https://pat-s.me/reproducibility-when-going-parallel/ for some helpful information)

```{r parallel-1}
# Create a function to process matrices to a Seurat object
create_seurat <- function(x) {
    x <- tenx_load_qc(
        path_10x = paste0(
            "Datasets/",
            x$gse, "/",
            x$id),
        violin_plot = FALSE,
        sample_name = x$id
    )
    return(x)
}

# Split the tibble into a list of vectors
geo_data_2 <- geo_data %>%
    as.data.frame() %>%
    split(1:nrow(geo_data)) %>%
    `names<-`(geo_data$id)

# Set the start time
message("Starting serial processing for loading/creating Seurat objects...")
t <- c(Sys.time())

# Map the function onto the list of vectors
tumors_serial <- lapply(geo_data_2, create_seurat)

# Set the completion time for above and start time for below
message("Moving on to parallel processing for loading/creating Seurat objects...")
t[2] <- Sys.time()

# Now repeat the function using mclapply
tumors_parallel <- mclapply(geo_data_2, create_seurat, mc.cores = 10L)

# Mark completion time for the parallel operation and calculate processing
message("Done.")
t[3] <- Sys.time()
print(paste("Serial processing time:",
    difftime(t[2], t[1], units = "secs"),
    "seconds"))
print(paste("Parallel processing time:",
    difftime(t[3], t[2], units = "secs"),
    "seconds"))

# Test to see of the two results are the same
print("Are the objects the same?")
identical(tumors_serial, tumors_parallel)

# Create a function to process from normalize to umap
process_seurat <- function(x) {
    x <- x %>%
        NormalizeData(verbose = FALSE) %>%
        FindVariableFeatures(verbose = FALSE) %>%
        ScaleData(verbose = FALSE) %>%
        RunPCA(verbose = FALSE) %>%
        FindNeighbors(verbose = FALSE) %>%
        FindClusters(verbose = FALSE) %>%
        RunUMAP(dims = 1:20, verbose = FALSE)
    return(x)
}

# Set the time for starting the serial processing
message("Starting serial processing of the Seurat objects...")
t[4] <- Sys.time()

# Map the function to the list of Seurat objects using lapply
tumors_serial <- lapply(tumors_serial, process_seurat)

# Mark completion of the previous operation and start of the next
message("Starting parallel processing of the Seurat objects...")
t[5] <- Sys.time()

# Map the function using mclapply
tumors_parallel <- mclapply(tumors_serial, process_seurat, mc.cores = 10L)

# Mark completion and calculate times
message("Done.")
t[6] <- Sys.time()
print(paste("Serial processing time:",
    difftime(t[5], t[4], units = "secs"),
    "seconds"))
print(paste("Parallel processing time:",
    difftime(t[6], t[5], units = "secs"),
    "seconds"))

# Are these two objects identical?
print("Are these two objects identical?")
identical(tumors_serial, tumors_parallel)

# Compare jobs run within a single operation
single_op <- function(x) {
    id <- x$id
    message(paste(id, "is starting..."))
    x <- create_seurat(x) %>%
        process_seurat()
    message(paste(id, "has completed."))
    return(x)
}

message("Starting the combined creating/processing approach...")
t[7] <- Sys.time()

tumors_parallel_2 <- mclapply(geo_data_2, single_op, mc.cores = 10L)

t[8] <- Sys.time()

print(paste("Total time for create, then process:",
    difftime(t[3], t[2], units = "secs") + difftime(t[6], t[5], units = "secs"),
    "seconds"))
print(paste("Total time for create + process in a single step:",
    difftime(t[8], t[7], units = "secs"),
    "seconds"))
```

## Practice 2: Be a good doobie (time permitting)

***A NOTE ABOUT COMPUTATIONAL STEWARDSHIP***
For the purposes of this course, we are performing parallel computing operations using an interactive session, to which we've assigned several computing resources. Generally, this is a very inefficient way to utilize resources, because you are really only using all of the requested CPUs for brief bursts of activity. The rest of the time, those cores sit idle while you write your code, but are not available for others to use. Leaving idle sessions like this running for long periods of time is poor form on a resource that is free to you (like Franklin) and a good way to spend a lot of money on computing power that you're not actually using. 

While interactive sessions like this can be helpful for development and debugging, they should generally be avoided. Also, when using an interactive session requesting multiple computing nodes, you should try to limit the number of nodes requested to those that you actually need and limit the time that you maintain the allocation (ie, close the session when you are done).

So, is there a way to be more efficient AND be a good citizen of our cyberspace?

YES! Here are some options:
1. Break your code down into sections that can be run as batch submissions through slurm.
2. Automate the above using the rslurm package. Run your interactive session on a single core (only requesting a single core in your srun or salloc interactive session scheduling request).
3. Find a middle ground. Requesting 3 cores will still speed up your data-intense operations about 3-fold, but leaves a lot more computing resources for others. (Or costs a lot less if you're paying for your wall hours.)
4. Make sure you turn things on and off to reduce your footprint. Use srun ... R to start your R sessions

Try one of these potential solutions above and see how it affects performace using benchmarks.

```{r parallel-slurm-r}
# Run the same block of code above as a slurm batch submission using rslurm
library(rslurm)

# Mark completion of the previous operation and start of the next
message("Starting parallel processing of the Seurat objects...")
t[9] <- Sys.time()

# Map the function using slurm_map
slurm_job <- slurm_map(tumors_serial,
    process_seurat,
    nodes = 1,
    cpus_per_node = length(tumors_serial))
tumors_parallel_3 <- get_slurm_out(slurm_job,
    outtype = "raw",
    wait = TRUE)

# Mark completion and calculate times
message("Done.")
t[10] <- Sys.time()
print(paste("Parallel processing time - mclapply:",
    difftime(t[5], t[4], units = "secs"),
    "seconds"))
print(paste("Parallel processing time - slurm_map:",
    difftime(t[6], t[5], units = "secs"),
    "seconds"))
```

## Discussion of pseudotime analysis/trajectory inference

This project gives a brief introduction to the world of pseudotime and trajectory methodologies. The large number of tools that have been developed to address this problem highlights the fact that this is a challenge with an array of imperfect solutions. Each approach provides solutions that address at least one type of problem, while no solution works well for every problem. Finding "true" trajectories often requires some trial and error using multiple different algorithms, which can often be complimentary.

A very nice comparative evaluation of the different approaches to pseudotime/trajectory analysis has been published by Saelens et al (https://doi.org/10.1038/s41587-019-0071-9). It really is worth a read if you want to understand more. You might also check out the metapackage the authors built to support their comparative benchmarking (https://github.com/dynverse/dyno). It's a little outdated at this point, but they did a good job putting it together.

Once weakness in nearly all of these approaches is that trajectories are almost entirely inferential. They should be viewed as different ways to "connect the dots," but they are NOT clear evidence that one cell type/cluster gives rise to another. Moreover, while some algorithms will generate educated guesses as to the directionality in the different connections between clusters, inferences of origin and destination are really just guesses. 

One can often use techniques such as RNA Velocity Analysis to get more direct evidence for directionality and evolutionary relationships. Indeed, these two techniques should be viewed as being highly complimentary. We will not have time to dive into velocity analysis today (maybe next year), but the following resources are a good place to start if you'd like to learn more:
 - https://doi.org/10.1371/journal.pcbi.1010492
 - https://doi.org/10.15252/msb.202110282
 - https://scvelo.readthedocs.io/en/stable/


## Session challenge

To take on today's challenge, install the dyno package* and then use it to compare results of several different approaches to trajectory inference using the same object(s). The data could be the same dataset used in class today, one taken from a repository, or one of your own. To qualify for judging, you must generate at least 8 distinct plots/analyses that evaluate different trajectories or gene expression modules that vary with pseudotime. Winners will be selected based on the following principles:
 - how appropriate the analyses are for addressing the problem/question
 - how creative the approach is
 - how well the approach is documented
 - how beautifully/accessibly the results are presented

To submit your code for judging, just save a new file titled "Session10-Challenge-Ryan_Roberts.Rmd" (replacing my name with yours, obviously) in the "Challenges" folder on the SCRGOT Coder Upgrade OneDrive.

*NOTE: Running the dyno package requires singularity. Singularity joined forces with the Linux foundation and became Apptainer, which is already installed on the Franklin cluster. When you run singularity commands, these will be automatically run by apptainer, and it works fine. However, if you run test_singularity_installation and use the "detailed = TRUE" option, it will tell you that the version is old. Just ignore this. Any apptainer installation is equivalent to a singularity version >3.8.

```{r}
system("ml load GCCcore/9.3.0 ImageMagick")
library(dyno)

# Set up the dyno object from the Seurat data
# First, add the raw and normalized counts
dyn_obj <- wrap_expression(
    expression = GetAssayData(seurat_obj, slot = "data", assay = "RNA") %>% t(),
    counts = GetAssayData(seurat_obj, slot = "counts", assay = "RNA") %>% t())

# Second, add the clustering information
dyn_obj <- add_grouping(dyn_obj, seurat_obj$assignment)
 
# And the UMAP embeddings
dyn_obj <- add_dimred(dyn_obj, Embeddings(seurat_obj[["umap"]]))

# And the starting cell
# Interactive steps run previously:
# CellSelector(DimPlot(seurat_obj))
dyn_obj <- add_prior_information(dyn_obj, 
    start_id = "OS_3_GTTGAACTCTATCGTT-1")

# Use the interactive shiny tool to explore the different methods and choose optimal
picked <- guidelines_shiny(dyn_obj)

# Run the trajectory inferrence algorithms

```